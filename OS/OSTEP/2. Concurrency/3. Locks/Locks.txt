Locks

	Programmers annotate source code with locks, putting them around critical sections, and thus ensure that any such section executes as if it were a single atomic instruction.


	Locks: The Basic Idea

		To use a lock, we add some code around the critical section like this:

			lock_t mutex; // some globally-allocated lock 'mutex'
			...
			lock(&mutex);
			balance = balance + 1;
			unlock(&mutex);

		The lock variable holds the state of the lock at any instant in time. It is either available (unlocked or free) or acquired (locked or held), and thus exactly one thread holds the lock and presumably is in a critical section.

		Calling the routine lock() tries to acquire the lock; if no other thread holds the lock, the thread will acquire the lock and enter the critical section; this thread is sometimes said to be the owner of the lock. If another thread calls lock() on the same lock variable, it will not return while the thread is held by another thread; in this way, other threads are prevented from entering the critical section while the first thread that holds the lock is in there.

		Once the owner of the lock calls "unlock()", the lock will be free again. If threads are waiting, one of them will notice this change and will acquire the lock.


	Pthread Locks

		locks are called 'mutex' (mutual exclusion) in POSIX library. The POSIX code:

			pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
			
			Pthread_mutex_lock(&lock); // wrapper; exits on failure
			balance = balance + 1;
			Pthread_mutex_unlock(&lock);

		We can use different locks (variables) to protect different critical sections (fine-grained approach), we can also use a single lock to protect all the critical sections (coarse-grained approach), this decreases our concurrency.


	Building A Lock

		THE CRUX: HOW TO BUILD A LOCK

			How can we build an efficient lock? Efficient locks provide mutual exclusion at low cost, and also might attain a few other properties we discuss below. What hardware support is needed? What OS support?

		We can use the following criteria to evaluate a locking mechanism:

			- Mutual Exclusion: Basically, does the lock work, preventing multiple threads from entering a critical section?
			- Fairness: Does each thread get a fair shot at acquiring the lock? Does any thread starve while waiting for the lock?
			- Performance: What are the overheads involved in a single thread scenario? multi-thread, single CPU scenario, and multi-thread/CPU scenario?


		Controlling Interrupts

			The idea is to disable interrupts; this solution was invented for single-processor systems. The code would look like this:

				void lock() {
					DisableInterrupts();
				}

				void unlock() {
					EnableInterrupts();
				}

			Pros:
				- Simplicity

			Cons:
				- We allow the calling thread to perform a privileged operation. Malicious programs can use DisableInterrupts to take over the system.
				- This won't work on multi-processor systems.
				- Turning off interrupts can lead to interrupts becoming lost.
				- This isn't efficient. The code that masks and unmasks interrupts tends to be executed slowly by modern CPUs.

			But this approach is used in some cases by the OS. With the OS, the trust issue disappears.


		A Failed Attempt: Just Using Loads/Stores

			We reply on a single variable (flag) to implement mutual exclusion. 

			typedef struct __lock_t { int flag; } lock_t;
			
			void init(lock_t *mutex) {
				// 0 -> lock is available, 1 -> held
				mutex->flag = 0;
			}
			
			void lock(lock_t *mutex) {
				while (mutex->flag == 1) // TEST the flag
					; // spin-wait (do nothing)
				mutex->flag = 1; // now SET it!
			}
			
			void unlock(lock_t *mutex) {
				mutex->flag = 0;
			}

			Pros: 
				- Simplicity

			Cons:
				- Correctness: Due to concurrency, multiple threads can acquire the lock. 
				- Performance: Waiting is done using "spin-waiting" which wastes the CPU cycle.
			 

		Building Working Spin Locks With Test-And-Set

			The simplest bit of hardware support for mutex is know as 'test-and-set' (or atomic-exachange) instruction. We define what test-and-set instruction does via the following C code snippet:

				int TestAndSet(int *old_ptr, int new) {
					int old = *old_ptr;
					*old_ptr = new;
					return old;
				}

			The sequence of operations in "test-and-set" is performed atomically. We can build a simple "spin-lock" with this hardware support.

			typedef struct __lock_t {
				int flag;
			} lock_t;
			
			void init(lock_t *lock) {
				// 0: lock is available, 1: lock is held
				lock->flag = 0;
			}
			
			void lock(lock_t *lock) {
				while (TestAndSet(&lock->flag, 1) == 1)
					; // spin-wait (do nothing)
			}
			
			void unlock(lock_t *lock) {
				lock->flag = 0;
			}

			To work correctly on a single processor, it requires a pre-emptive scheduler. Without a preemptive scheduler, a waiting thread spinning on the CPU won't relinquish it.


		Evaluating Spin Locks

			Correctness: It's evident that the "test-and-set" spinlocks provide mutual exclusion.

			Fairness: They don't have any mechanism to guarantee fairness, therefore threads might starve waiting for the lock.

			Performance: They aren't performant since the waiting threads waste the CPU cycle. Imagine a scenario where the thread which owns the lock gets interrupted. The scheduler might run every other thread, each of which tries to acquire the lock. In this case, each of these threads will spin the duration of a time slice.


		Compare-And-Swap

			Another hardware primitive that some systems offer is known as 'compare-and-swap' (or 'compare-and-exchange'). The pseudocode is as follows:

				int CompareAndSwap(int *ptr, int expected, int new) {
					int original = *ptr;
					if (original == expected)
						*ptr = new;
					return original;
				}

			The mechanism is similar to 'test-and-set', the lock() routine is as follows:

				void lock(lock_t *lock) {
					while (CompareAndSwap(&lock->flag, 0, 1) == 1)
						; // spin
				}


		Load-Linked and Store-Conditional

			The load-linked operates much like a typical load instruction, and simply fetches a value from memory and places it in the register. The key difference comes with the store-conditional, which only succeeds (and updates the value stored at the address just load-linked from) if no intervening store to the address has taken place.

			int LoadLinked(int *ptr) {
				return *ptr;
			}
			
			int StoreConditional(int *ptr, int value) {
				if (no update to *ptr since LoadLinked to this address) {
					*ptr = value;
					return 1; // success!
				} else {
					return 0; // failed to update
				}
			}

			void lock(lock_t *lock) {
				while (1) {
					while (LoadLinked(&lock->flag) == 1)
						; // spin until it’s zero
					if (StoreConditional(&lock->flag, 1) == 1)
						return; // if set-it-to-1 was a success: all done
					// otherwise: try it all over again
				}
			}
			
			void unlock(lock_t *lock) {
				lock->flag = 0;
			}


			void lock(lock_t *lock) {
				while (LoadLinked(&lock->flag) || !StoreConditional(&lock->flag, 1))
					; // spin
			}


		Fetch And Add

			"fetch-and-add" instruction is a hardware primitive which automatically increments a value while returning the old value at a particular address.
			The C pseudocode is as follows:

				int FetchAndAdd(int *ptr) {
					int old = *ptr;
					*ptr = old + 1;
					return old;
				}

			The locking mechanism is as follows:

				typedef struct __lock_t {
					int ticket;
					int turn;
				} lock_t;
				
				void lock_init(lock_t *lock) {
					lock->ticket = 0;
					lock->turn = 0;
				}
				
				void lock(lock_t *lock) {
					int myturn = FetchAndAdd(&lock->ticket);
					while (lock->turn != myturn)
						; // spin
				}
				
				void unlock(lock_t *lock) {
					lock->turn = lock->turn + 1;
				}

			This is a very simple approach that guarantees correctness and fairness.


	Too Much Spinning: What Now?

		Our simple hardware-based locks are simple and correct, but they aren't performant. CPU cycles are wasted by waiting threads.


		THE CRUX: HOW TO AVOID SPINNING

			How can we develop a lock that doesn’t needlessly waste time spinning on the CPU?


	A Simple Approach: Just Yield, Baby

		In this approach, we assume an OS primitive yield() which a thread can call when it wants to give up the CPU and let another thread run.
		Yield() is simply a system call that moves the caller from the 'running' state to the 'ready' state. Thus, the yielding thread essentially de-schedules itself.

		This approach improves performance but doesn't address the problem of fairness.


	Using Queues: Sleeping Instead Of Spinning

		We must explicitly exert some control over which thread next gets to acquire the lock after the current holder releases it. To do this, we need a little more OS support, as well as a queue to keep track of which threads are waiting to acquire the lock.

		For simplicity; we will use the support provided by Solaris:

			1. park(): To put a calling thread to sleep
		    2. unpark(threadID): to wake up a particular thread designated by "threadID".

	    Code:

	    	typedef struct __lock_t {
				int flag;
				int guard;
				queue_t *q;
			} lock_t;
			
			void lock_init(lock_t *m) {
				m->flag = 0;
				m->guard = 0;
				queue_init(m->q);
			}
			
			void lock(lock_t *m) {
				while (TestAndSet(&m->guard, 1) == 1)
					; //acquire guard lock by spinning
				if (m->flag == 0) {
					m->flag = 1; // lock is acquired
					m->guard = 0;
				} else {
					queue_add(m->q, gettid());
					m->guard = 0;
					park();
				}
			}
			
			void unlock(lock_t *m) {
				while (TestAndSet(&m->guard, 1) == 1)
					; //acquire guard lock by spinning
				if (queue_empty(m->q))
					m->flag = 0; // let go of lock; no one wants it
				else
					unpark(queue_remove(m->q)); // hold lock
				// (for next thread!)
				m->guard = 0;
			}


		This approach doesn't avoid spin-waiting entirely; a thread might be interrupted while acquiring or releasing a lock, and thus cause other threads to spin-wait for this one to run again. However, the time spent spinning is limited.

		When a thread is woken up, it will be as if it is returning from "park()"; however, it does not hold the guard at that point in the code and thus cannot even try to set the flag to one. Thus we just pass the lock from the thread releasing it to the next thread acquiring it; the flag is not set to 0 in-between.

		With just the wrong timing, a thread will be about to park, assuming that it should sleep until the lock is no longer held. A switch at that time to a thread releasing the lock would lead to the former sleeping forever, a problem sometimes called 'wakeup/waiting race'.

		Solaris solves this problem by a third system call: setpark(). By calling this, a thread can indicate that it is about to park. If it happens to be interrupted and another thread calls unpark before park is actually called, the subsequent park returns immediately instead of sleeping. The code modification inside of lock() is quite small:

			queue_add(m->q, gettid());
			setpark(); // new code
			m->guard = 0;


	Different OS, Different Support

		Linux provides "futex", which is similar to the Solaris interface but provides more in-kernel functionality. The Futex is a 'two-phase' lock, it realized that spinning can be useful, particularly if the lock is about to be released. So, in the first phase, the lock spins for a while, hoping that it can acquire the lock. If it fails to acquire, the thread enters the second phase, where the caller is put to sleep, and only woken up when the lock becomes free later.
